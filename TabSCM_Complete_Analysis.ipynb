{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# TabSCM: Zero-Shot Causal Discovery with Transformers\n",
                "## Complete Project Analysis & Improvement Roadmap\n",
                "\n",
                "**Goal**: Learn to construct Structural Causal Models (SCMs) from tabular data in a zero-shot manner.\n",
                "\n",
                "**Author**: Research Team  \n",
                "**Date**: November 2025  \n",
                "**Status**: ⚠️ Model Not Learning (AUROC ≈ 0.5, F1 = 0.0)\n",
                "\n",
                "---\n",
                "\n",
                "## Table of Contents\n",
                "1. [Problem Definition](#1-problem-definition)\n",
                "2. [Approach Overview](#2-approach-overview)\n",
                "3. [Data Generation](#3-data-generation)\n",
                "4. [Model Architecture](#4-model-architecture)\n",
                "5. [Training Pipeline](#5-training-pipeline)\n",
                "6. [Experiments & Results](#6-experiments--results)\n",
                "7. [Failure Analysis](#7-failure-analysis)\n",
                "8. [Improvement Suggestions](#8-improvement-suggestions)\n",
                "9. [Next Steps](#9-next-steps)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. Problem Definition\n",
                "\n",
                "### What is Causal Discovery?\n",
                "Given observational and interventional data, infer the **causal graph** (DAG) that generated the data.\n",
                "\n",
                "**Example**:\n",
                "```\n",
                "Data: X₁, X₂, X₃\n",
                "True Graph: X₁ → X₂ → X₃\n",
                "Goal: Predict adjacency matrix:\n",
                "     [0 1 0]\n",
                "     [0 0 1]\n",
                "     [0 0 0]\n",
                "```\n",
                "\n",
                "### Why Zero-Shot?\n",
                "Traditional methods (PC, GES, NOTEARS) require:\n",
                "- Large amounts of data per graph\n",
                "- Assumptions about mechanisms (linearity, Gaussianity)\n",
                "\n",
                "**Our Approach**: Train a Transformer on millions of synthetic SCMs, then apply to new graphs without retraining.\n",
                "\n",
                "### Key Challenge\n",
                "**Sparsity**: Most variable pairs are NOT causally connected (~70% non-edges)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Approach Overview\n",
                "\n",
                "### Pipeline\n",
                "```\n",
                "1. Generate Random DAG (8-128 nodes)\n",
                "2. Assign Causal Mechanisms (Linear, MLP, Sine, Tanh)\n",
                "3. Simulate Observational Data (1000 samples)\n",
                "4. Simulate Interventions (do(X_i = c))\n",
                "5. Stack Data + Masks → Input Tensor\n",
                "6. Feed to Transformer → Predict Adjacency Matrix\n",
                "```\n",
                "\n",
                "### Architecture\n",
                "```\n",
                "Input: (batch, rows, cols)\n",
                "  ↓\n",
                "SetEmbedding: Encode each column as a set\n",
                "  ↓\n",
                "Transformer: Capture interactions\n",
                "  ↓\n",
                "Bilinear Head: Predict P(i → j)\n",
                "  ↓\n",
                "Output: (batch, cols, cols) adjacency matrix\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Data Generation\n",
                "\n",
                "### 3.1 Random DAG Generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import networkx as nx\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Generate a random DAG\n",
                "def generate_random_dag(n_nodes=10, p_edge=0.3):\n",
                "    \"\"\"Generate Erdős-Rényi DAG\"\"\"\n",
                "    graph = nx.DiGraph()\n",
                "    graph.add_nodes_from(range(n_nodes))\n",
                "    \n",
                "    # Add edges only from i to j where i < j (ensures acyclicity)\n",
                "    for i in range(n_nodes):\n",
                "        for j in range(i + 1, n_nodes):\n",
                "            if np.random.rand() < p_edge:\n",
                "                graph.add_edge(i, j)\n",
                "    \n",
                "    return graph\n",
                "\n",
                "# Example\n",
                "G = generate_random_dag(n_nodes=8, p_edge=0.3)\n",
                "pos = nx.spring_layout(G, seed=42)\n",
                "nx.draw(G, pos, with_labels=True, node_color='lightblue', \n",
                "        node_size=500, arrowsize=20, font_size=12)\n",
                "plt.title(f\"Random DAG ({G.number_of_nodes()} nodes, {G.number_of_edges()} edges)\")\n",
                "plt.show()\n",
                "\n",
                "print(f\"Adjacency Matrix:\\n{nx.to_numpy_array(G).astype(int)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.2 Causal Mechanisms\n",
                "\n",
                "We support 4 types of mechanisms:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def simulate_scm(graph, n_samples=1000, mechanism_type='linear'):\n",
                "    \"\"\"Simulate data from SCM\"\"\"\n",
                "    n_nodes = graph.number_of_nodes()\n",
                "    X = np.zeros((n_samples, n_nodes))\n",
                "    \n",
                "    # Topological order ensures parents are computed before children\n",
                "    topo_order = list(nx.topological_sort(graph))\n",
                "    \n",
                "    for node in topo_order:\n",
                "        parents = list(graph.predecessors(node))\n",
                "        noise = np.random.normal(0, 0.1, size=n_samples)\n",
                "        \n",
                "        if len(parents) == 0:\n",
                "            # Root node: sample from prior\n",
                "            X[:, node] = np.random.normal(0, 1, size=n_samples)\n",
                "        else:\n",
                "            parent_data = X[:, parents]\n",
                "            weights = np.random.uniform(-2, 2, size=len(parents))\n",
                "            \n",
                "            if mechanism_type == 'linear':\n",
                "                X[:, node] = np.dot(parent_data, weights) + noise\n",
                "            elif mechanism_type == 'tanh':\n",
                "                X[:, node] = np.tanh(np.dot(parent_data, weights)) + noise\n",
                "            elif mechanism_type == 'sine':\n",
                "                X[:, node] = np.sin(np.dot(parent_data, weights)) + noise\n",
                "    \n",
                "    return X\n",
                "\n",
                "# Example: Linear vs. Non-Linear\n",
                "X_linear = simulate_scm(G, mechanism_type='linear')\n",
                "X_tanh = simulate_scm(G, mechanism_type='tanh')\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "axes[0].scatter(X_linear[:, 0], X_linear[:, 1], alpha=0.5)\n",
                "axes[0].set_title('Linear Mechanism')\n",
                "axes[0].set_xlabel('X₀')\n",
                "axes[0].set_ylabel('X₁')\n",
                "\n",
                "axes[1].scatter(X_tanh[:, 0], X_tanh[:, 1], alpha=0.5)\n",
                "axes[1].set_title('Tanh Mechanism')\n",
                "axes[1].set_xlabel('X₀')\n",
                "axes[1].set_ylabel('X₁')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.3 Interventional Data\n",
                "\n",
                "**Intervention**: Set a variable to a fixed value and observe downstream effects."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def simulate_intervention(graph, target_node, intervention_value=5.0, n_samples=1000):\n",
                "    \"\"\"Simulate do(X_target = value)\"\"\"\n",
                "    n_nodes = graph.number_of_nodes()\n",
                "    X = np.zeros((n_samples, n_nodes))\n",
                "    \n",
                "    topo_order = list(nx.topological_sort(graph))\n",
                "    \n",
                "    for node in topo_order:\n",
                "        if node == target_node:\n",
                "            # Intervention: fix value\n",
                "            X[:, node] = intervention_value\n",
                "        else:\n",
                "            parents = list(graph.predecessors(node))\n",
                "            noise = np.random.normal(0, 0.1, size=n_samples)\n",
                "            \n",
                "            if len(parents) == 0:\n",
                "                X[:, node] = np.random.normal(0, 1, size=n_samples)\n",
                "            else:\n",
                "                parent_data = X[:, parents]\n",
                "                weights = np.random.uniform(-2, 2, size=len(parents))\n",
                "                X[:, node] = np.dot(parent_data, weights) + noise\n",
                "    \n",
                "    return X\n",
                "\n",
                "# Example: Intervene on node 0\n",
                "X_obs = simulate_scm(G)\n",
                "X_int = simulate_intervention(G, target_node=0, intervention_value=5.0)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "axes[0].hist(X_obs[:, 0], bins=30, alpha=0.7, label='Observational')\n",
                "axes[0].set_title('Node 0: Observational')\n",
                "axes[0].set_xlabel('Value')\n",
                "\n",
                "axes[1].hist(X_int[:, 0], bins=30, alpha=0.7, label='Interventional', color='orange')\n",
                "axes[1].set_title('Node 0: do(X₀ = 5)')\n",
                "axes[1].set_xlabel('Value')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Model Architecture\n",
                "\n",
                "### 4.1 SetEmbedding\n",
                "\n",
                "**Idea**: Treat each column as a **set** of (value, mask) pairs.\n",
                "\n",
                "**Why?** Variables have different numbers of samples, and we need permutation invariance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "class SetEmbedding(nn.Module):\n",
                "    def __init__(self, embed_dim=128):\n",
                "        super().__init__()\n",
                "        # Encode each (value, mask) pair\n",
                "        self.element_encoder = nn.Sequential(\n",
                "            nn.Linear(2, embed_dim),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(embed_dim, embed_dim),\n",
                "            nn.ReLU()\n",
                "        )\n",
                "        # Post-pooling MLP\n",
                "        self.post_pool_mlp = nn.Sequential(\n",
                "            nn.Linear(embed_dim, embed_dim),\n",
                "            nn.LayerNorm(embed_dim)\n",
                "        )\n",
                "\n",
                "    def forward(self, x, m):\n",
                "        # x: (batch, rows, 1) - values\n",
                "        # m: (batch, rows, 1) - intervention masks\n",
                "        combined = torch.stack([x, m], dim=-1)  # (batch, rows, 2)\n",
                "        encodings = self.element_encoder(combined)  # (batch, rows, embed_dim)\n",
                "        pooled = torch.mean(encodings, dim=1)  # (batch, embed_dim)\n",
                "        return self.post_pool_mlp(pooled)\n",
                "\n",
                "# Example\n",
                "set_emb = SetEmbedding(embed_dim=64)\n",
                "x_example = torch.randn(4, 100, 1)  # 4 batches, 100 rows, 1 column\n",
                "m_example = torch.zeros(4, 100, 1)  # No interventions\n",
                "output = set_emb(x_example, m_example)\n",
                "print(f\"Input shape: {x_example.shape}\")\n",
                "print(f\"Output shape: {output.shape}\")  # (4, 64)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 Full Model: ZCIA Transformer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ZCIA_Transformer(nn.Module):\n",
                "    def __init__(self, max_cols=128, embed_dim=128, n_heads=4, n_layers=4):\n",
                "        super().__init__()\n",
                "        self.embed_dim = embed_dim\n",
                "        \n",
                "        # Step 1: Embed each column\n",
                "        self.set_encoder = SetEmbedding(embed_dim)\n",
                "        \n",
                "        # Step 2: Transformer to capture interactions\n",
                "        encoder_layer = nn.TransformerEncoderLayer(\n",
                "            d_model=embed_dim, \n",
                "            nhead=n_heads, \n",
                "            batch_first=True,\n",
                "            dim_feedforward=embed_dim*4,\n",
                "            dropout=0.1\n",
                "        )\n",
                "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
                "        \n",
                "        # Step 3: Predict edges\n",
                "        self.head = nn.Bilinear(embed_dim, embed_dim, 1)\n",
                "\n",
                "    def forward(self, x, m, pad_mask):\n",
                "        # x: (batch, rows, cols)\n",
                "        # m: (batch, rows, cols)\n",
                "        # pad_mask: (batch, cols) - True for padded columns\n",
                "        \n",
                "        batch_size, n_rows, n_cols = x.shape\n",
                "        \n",
                "        # Flatten to treat each column independently\n",
                "        x_flat = x.reshape(-1, n_rows)  # (batch*cols, rows)\n",
                "        m_flat = m.reshape(-1, n_rows)\n",
                "        \n",
                "        # Encode each column\n",
                "        col_embeddings_flat = self.set_encoder(x_flat.unsqueeze(-1), m_flat.unsqueeze(-1))\n",
                "        col_embeddings = col_embeddings_flat.reshape(batch_size, n_cols, self.embed_dim)\n",
                "        \n",
                "        # Transformer attention\n",
                "        z = self.transformer(col_embeddings, src_key_padding_mask=pad_mask)\n",
                "        \n",
                "        # Predict edges: P(i → j)\n",
                "        z_i = z.unsqueeze(2).expand(-1, -1, n_cols, -1)  # (batch, cols, cols, embed_dim)\n",
                "        z_j = z.unsqueeze(1).expand(-1, n_cols, -1, -1)\n",
                "        logits = self.head(z_i, z_j).squeeze(-1)  # (batch, cols, cols)\n",
                "        \n",
                "        return logits\n",
                "\n",
                "# Example\n",
                "model = ZCIA_Transformer(max_cols=10, embed_dim=64, n_heads=4, n_layers=2)\n",
                "x = torch.randn(2, 100, 10)  # 2 batches, 100 rows, 10 columns\n",
                "m = torch.zeros(2, 100, 10)\n",
                "pad_mask = torch.zeros(2, 10, dtype=torch.bool)\n",
                "\n",
                "logits = model(x, m, pad_mask)\n",
                "print(f\"Input shape: {x.shape}\")\n",
                "print(f\"Output shape: {logits.shape}\")  # (2, 10, 10) - adjacency matrices\n",
                "print(f\"\\nSample logits (first batch):\\n{logits[0].detach().numpy()[:5, :5]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Training Pipeline\n",
                "\n",
                "### 5.1 Loss Function: Weighted BCE"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_masked_loss(logits, target, pad_mask, pos_weight=None):\n",
                "    \"\"\"\n",
                "    Compute loss only on valid (non-padded) entries.\n",
                "    \n",
                "    Args:\n",
                "        logits: (batch, cols, cols) - predicted edge probabilities (logits)\n",
                "        target: (batch, cols, cols) - ground truth adjacency matrix\n",
                "        pad_mask: (batch, cols) - True for padded columns\n",
                "        pos_weight: Weight for positive class (edges)\n",
                "    \"\"\"\n",
                "    # Create valid matrix: 1 where both i and j are valid\n",
                "    valid_cols = ~pad_mask  # (batch, cols)\n",
                "    valid_matrix = torch.einsum('bi,bj->bij', valid_cols.float(), valid_cols.float())\n",
                "    \n",
                "    # Weighted BCE loss\n",
                "    if pos_weight is not None:\n",
                "        criterion = nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight)\n",
                "    else:\n",
                "        criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
                "        \n",
                "    loss_matrix = criterion(logits, target)\n",
                "    masked_loss = loss_matrix * valid_matrix\n",
                "    \n",
                "    return masked_loss.sum() / (valid_matrix.sum() + 1e-6)\n",
                "\n",
                "# Example\n",
                "logits = torch.randn(2, 10, 10)\n",
                "target = torch.randint(0, 2, (2, 10, 10)).float()\n",
                "pad_mask = torch.zeros(2, 10, dtype=torch.bool)\n",
                "pad_mask[0, 8:] = True  # Pad last 2 columns in first batch\n",
                "\n",
                "# Without weighting\n",
                "loss_unweighted = compute_masked_loss(logits, target, pad_mask)\n",
                "print(f\"Unweighted loss: {loss_unweighted.item():.4f}\")\n",
                "\n",
                "# With weighting (edges are 2.33x more important)\n",
                "pos_weight = torch.tensor([2.33])\n",
                "loss_weighted = compute_masked_loss(logits, target, pad_mask, pos_weight=pos_weight)\n",
                "print(f\"Weighted loss: {loss_weighted.item():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 Gradient Accumulation\n",
                "\n",
                "**Problem**: Batch size 32 doesn't fit in 24GB VRAM.  \n",
                "**Solution**: Accumulate gradients over 4 mini-batches of size 8."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pseudocode for gradient accumulation\n",
                "\"\"\"\n",
                "optimizer.zero_grad()\n",
                "\n",
                "for step in range(total_steps):\n",
                "    batch = next(dataloader)\n",
                "    \n",
                "    # Forward pass\n",
                "    logits = model(batch['x'], batch['m'], batch['pad_mask'])\n",
                "    loss = compute_loss(logits, batch['y']) / accumulation_steps\n",
                "    \n",
                "    # Backward pass\n",
                "    loss.backward()\n",
                "    \n",
                "    # Update weights every N steps\n",
                "    if (step + 1) % accumulation_steps == 0:\n",
                "        optimizer.step()\n",
                "        optimizer.zero_grad()\n",
                "\"\"\"\n",
                "\n",
                "print(\"Effective batch size = batch_size × accumulation_steps\")\n",
                "print(\"Example: 8 × 4 = 32\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Experiments & Results\n",
                "\n",
                "### 6.1 Baseline (No Optimizations)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Results from actual runs\n",
                "results = {\n",
                "    'Experiment': ['Baseline', 'Weighted Loss + Curriculum', 'Gradient Accumulation', 'Mixed Precision (AMP)'],\n",
                "    'AUROC': [0.4998, 0.5352, 0.5008, 'NaN'],\n",
                "    'F1': [0.0000, 0.0000, 0.0000, 'N/A'],\n",
                "    'Loss': [0.3041, 0.5401, 0.1477, 'NaN'],\n",
                "    'Status': ['❌ Failed', '⚠️ Slight improvement', '❌ Failed', '❌ Crashed']\n",
                "}\n",
                "\n",
                "df = pd.DataFrame(results)\n",
                "print(df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.2 Visualization of Failure\n",
                "\n",
                "Let's visualize what the model is predicting:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import seaborn as sns\n",
                "\n",
                "# Simulate model predictions (all near 0)\n",
                "ground_truth = np.array([\n",
                "    [0, 1, 0, 1, 0],\n",
                "    [0, 0, 1, 0, 0],\n",
                "    [0, 0, 0, 1, 0],\n",
                "    [0, 0, 0, 0, 1],\n",
                "    [0, 0, 0, 0, 0]\n",
                "])\n",
                "\n",
                "predictions = np.random.uniform(0, 0.1, (5, 5))  # Model predicts ~0 everywhere\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
                "\n",
                "sns.heatmap(ground_truth, annot=True, cmap='Blues', cbar=False, ax=axes[0], vmin=0, vmax=1)\n",
                "axes[0].set_title('Ground Truth Adjacency Matrix')\n",
                "axes[0].set_xlabel('To')\n",
                "axes[0].set_ylabel('From')\n",
                "\n",
                "sns.heatmap(predictions, annot=True, fmt='.2f', cmap='Reds', cbar=False, ax=axes[1], vmin=0, vmax=1)\n",
                "axes[1].set_title('Model Predictions (All Near 0)')\n",
                "axes[1].set_xlabel('To')\n",
                "axes[1].set_ylabel('From')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n⚠️ The model predicts very low probabilities for ALL edges.\")\n",
                "print(\"This minimizes loss (since most pairs are non-edges) but doesn't learn structure.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. Failure Analysis\n",
                "\n",
                "### 7.1 The Sparsity Trap\n",
                "\n",
                "**Problem**: Causal graphs are sparse (~30% edges, 70% non-edges).\n",
                "\n",
                "**Model Behavior**:\n",
                "- Predicting `P(edge) = 0` for everything gives low loss\n",
                "- Even with `pos_weight = 2.33`, the penalty for missing edges is too small\n",
                "\n",
                "**Math**:\n",
                "```\n",
                "Loss = -[p * log(σ(z)) * w + (1-p) * log(1 - σ(z))]\n",
                "\n",
                "If p_edge = 0.3:\n",
                "  - 30% of pairs: y = 1 (edge)\n",
                "  - 70% of pairs: y = 0 (non-edge)\n",
                "\n",
                "If model predicts z → -∞ (σ(z) ≈ 0):\n",
                "  - Edge loss: 0.3 * (-∞) * 2.33 = -∞ (bad)\n",
                "  - Non-edge loss: 0.7 * 0 = 0 (good)\n",
                "\n",
                "But in practice, z ≈ -5 gives:\n",
                "  - Edge loss: 0.3 * 5 * 2.33 ≈ 3.5\n",
                "  - Non-edge loss: 0.7 * 0.007 ≈ 0.005\n",
                "  - Total: 3.5 (acceptable)\n",
                "```\n",
                "\n",
                "The model finds a local minimum where it predicts low probabilities everywhere."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7.2 Architecture Limitations\n",
                "\n",
                "**SetEmbedding Issues**:\n",
                "1. **Mean Pooling Loses Information**: Averaging over rows discards distributional details\n",
                "2. **Weak Intervention Signal**: Binary masks (0 or 1) provide limited information\n",
                "3. **No Positional Encoding**: Transformer doesn't know variable ordering\n",
                "\n",
                "**Example**:\n",
                "```python\n",
                "# Two different distributions\n",
                "x1 = [1, 2, 3, 4, 5]  # Mean = 3\n",
                "x2 = [3, 3, 3, 3, 3]  # Mean = 3\n",
                "\n",
                "# After mean pooling, they look identical!\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7.3 Mixed Precision (AMP) Failure\n",
                "\n",
                "**What Happened**: Loss became NaN after a few steps.\n",
                "\n",
                "**Root Cause**:\n",
                "- FP16 has limited range: ~[-65504, 65504]\n",
                "- Weighted loss with `pos_weight = 2.33` creates large gradients\n",
                "- FP16 overflow → NaN\n",
                "\n",
                "**Solution**: Disabled AMP (use FP32 only)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize FP16 overflow\n",
                "fp16_max = 65504\n",
                "gradient_values = np.linspace(0, 100000, 1000)\n",
                "fp16_safe = np.clip(gradient_values, -fp16_max, fp16_max)\n",
                "\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.plot(gradient_values, label='True Gradient', linewidth=2)\n",
                "plt.plot(fp16_safe, label='FP16 Clipped', linestyle='--', linewidth=2)\n",
                "plt.axhline(fp16_max, color='red', linestyle=':', label='FP16 Max')\n",
                "plt.xlabel('Step')\n",
                "plt.ylabel('Gradient Value')\n",
                "plt.title('FP16 Overflow in Mixed Precision Training')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()\n",
                "\n",
                "print(\"⚠️ When gradients exceed 65504, FP16 overflows to NaN.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 8. Improvement Suggestions\n",
                "\n",
                "### 8.1 Basic Fixes (Immediate)\n",
                "\n",
                "#### Fix 1: Focal Loss\n",
                "**Problem**: Weighted BCE still allows model to ignore edges.  \n",
                "**Solution**: Focal Loss down-weights easy examples (non-edges)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class FocalLoss(nn.Module):\n",
                "    def __init__(self, alpha=0.25, gamma=2.0):\n",
                "        super().__init__()\n",
                "        self.alpha = alpha\n",
                "        self.gamma = gamma\n",
                "        \n",
                "    def forward(self, logits, targets):\n",
                "        bce = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
                "        pt = torch.exp(-bce)  # Probability of correct class\n",
                "        focal = self.alpha * (1 - pt) ** self.gamma * bce\n",
                "        return focal.mean()\n",
                "\n",
                "# Example\n",
                "focal_loss = FocalLoss(alpha=0.25, gamma=2.0)\n",
                "logits = torch.randn(10, 10)\n",
                "targets = torch.randint(0, 2, (10, 10)).float()\n",
                "loss = focal_loss(logits, targets)\n",
                "print(f\"Focal Loss: {loss.item():.4f}\")\n",
                "\n",
                "print(\"\\n✅ Focal Loss focuses on hard examples (edges) and ignores easy examples (non-edges).\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Fix 2: Increase Learning Rate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Current: lr = 1e-4\n",
                "# Recommended: lr = 1e-3 (10x increase)\n",
                "\n",
                "print(\"Current LR: 0.0001\")\n",
                "print(\"Recommended LR: 0.001\")\n",
                "print(\"\\nRationale: Model may be stuck in shallow local minimum.\")\n",
                "print(\"Higher LR can help escape.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Fix 3: Simplify Architecture\n",
                "**Problem**: SetEmbedding loses information via mean pooling.  \n",
                "**Solution**: Use simpler embedding."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SimpleEmbedding(nn.Module):\n",
                "    def __init__(self, embed_dim=128):\n",
                "        super().__init__()\n",
                "        self.encoder = nn.Linear(2, embed_dim)\n",
                "        \n",
                "    def forward(self, x, m):\n",
                "        # x: (batch, rows, cols)\n",
                "        # m: (batch, rows, cols)\n",
                "        \n",
                "        # Take mean over rows\n",
                "        x_mean = x.mean(dim=1)  # (batch, cols)\n",
                "        m_mean = m.mean(dim=1)  # (batch, cols)\n",
                "        \n",
                "        # Combine\n",
                "        combined = torch.stack([x_mean, m_mean], dim=-1)  # (batch, cols, 2)\n",
                "        return self.encoder(combined)  # (batch, cols, embed_dim)\n",
                "\n",
                "# Example\n",
                "simple_emb = SimpleEmbedding(embed_dim=64)\n",
                "x = torch.randn(4, 100, 10)\n",
                "m = torch.zeros(4, 100, 10)\n",
                "output = simple_emb(x, m)\n",
                "print(f\"Input: {x.shape} → Output: {output.shape}\")\n",
                "print(\"\\n✅ Simpler = fewer parameters = easier to train.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 8.2 Intermediate Fixes\n",
                "\n",
                "#### Fix 4: Auxiliary Task (Multi-Task Learning)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Add a secondary task: Predict which nodes are intervened\n",
                "\n",
                "class ZCIA_MultiTask(nn.Module):\n",
                "    def __init__(self, embed_dim=128):\n",
                "        super().__init__()\n",
                "        # ... (same as before)\n",
                "        \n",
                "        # Additional head for intervention prediction\n",
                "        self.intervention_head = nn.Linear(embed_dim, 1)\n",
                "        \n",
                "    def forward(self, x, m, pad_mask):\n",
                "        # ... (same as before)\n",
                "        z = self.transformer(col_embeddings, src_key_padding_mask=pad_mask)\n",
                "        \n",
                "        # Task 1: Edge prediction\n",
                "        edge_logits = self.head(z_i, z_j).squeeze(-1)\n",
                "        \n",
                "        # Task 2: Intervention prediction\n",
                "        intervention_logits = self.intervention_head(z).squeeze(-1)\n",
                "        \n",
                "        return edge_logits, intervention_logits\n",
                "\n",
                "print(\"Multi-Task Loss:\")\n",
                "print(\"  L_total = L_edge + λ * L_intervention\")\n",
                "print(\"  where λ = 0.1\")\n",
                "print(\"\\n✅ Auxiliary task provides additional supervision signal.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Fix 5: Data Augmentation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Augmentation strategies:\n",
                "\n",
                "def augment_scm(X, adj, augmentation_type='permute'):\n",
                "    if augmentation_type == 'permute':\n",
                "        # Randomly permute variable order\n",
                "        perm = np.random.permutation(X.shape[1])\n",
                "        X_aug = X[:, perm]\n",
                "        adj_aug = adj[perm][:, perm]\n",
                "        return X_aug, adj_aug\n",
                "    \n",
                "    elif augmentation_type == 'noise':\n",
                "        # Add Gaussian noise\n",
                "        noise = np.random.normal(0, 0.1, X.shape)\n",
                "        return X + noise, adj\n",
                "    \n",
                "    elif augmentation_type == 'dropout':\n",
                "        # Randomly drop edges\n",
                "        mask = np.random.rand(*adj.shape) > 0.1\n",
                "        adj_aug = adj * mask\n",
                "        return X, adj_aug\n",
                "\n",
                "print(\"Augmentation Types:\")\n",
                "print(\"  1. Permute: Shuffle variable order\")\n",
                "print(\"  2. Noise: Add Gaussian noise to data\")\n",
                "print(\"  3. Dropout: Randomly remove edges\")\n",
                "print(\"\\n✅ Augmentation improves generalization.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Fix 6: Progressive Curriculum Learning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Instead of jumping to 100% linear, gradually increase complexity\n",
                "\n",
                "def get_p_linear(step, total_steps):\n",
                "    \"\"\"Gradually decrease p_linear from 1.0 to 0.5\"\"\"\n",
                "    progress = step / total_steps\n",
                "    return 1.0 - 0.5 * progress\n",
                "\n",
                "# Example\n",
                "steps = np.linspace(0, 20000, 100)\n",
                "p_linear_schedule = [get_p_linear(s, 20000) for s in steps]\n",
                "\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.plot(steps, p_linear_schedule, linewidth=2)\n",
                "plt.xlabel('Training Step')\n",
                "plt.ylabel('p_linear (Probability of Linear Mechanism)')\n",
                "plt.title('Progressive Curriculum Learning')\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.axhline(1.0, color='green', linestyle=':', label='100% Linear (Easy)')\n",
                "plt.axhline(0.5, color='red', linestyle=':', label='50% Linear (Hard)')\n",
                "plt.legend()\n",
                "plt.show()\n",
                "\n",
                "print(\"✅ Start easy (linear only), gradually add complexity.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 8.3 Advanced Fixes\n",
                "\n",
                "#### Fix 7: Graph Attention Networks (GAT)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Replace standard Transformer with GAT\n",
                "\n",
                "class GraphAttentionLayer(nn.Module):\n",
                "    def __init__(self, in_dim, out_dim):\n",
                "        super().__init__()\n",
                "        self.W = nn.Linear(in_dim, out_dim, bias=False)\n",
                "        self.a = nn.Linear(2 * out_dim, 1, bias=False)\n",
                "        \n",
                "    def forward(self, h):\n",
                "        # h: (batch, nodes, in_dim)\n",
                "        Wh = self.W(h)  # (batch, nodes, out_dim)\n",
                "        \n",
                "        # Compute attention scores\n",
                "        batch_size, n_nodes, out_dim = Wh.shape\n",
                "        Wh_i = Wh.unsqueeze(2).expand(-1, -1, n_nodes, -1)\n",
                "        Wh_j = Wh.unsqueeze(1).expand(-1, n_nodes, -1, -1)\n",
                "        concat = torch.cat([Wh_i, Wh_j], dim=-1)\n",
                "        \n",
                "        e = self.a(concat).squeeze(-1)  # (batch, nodes, nodes)\n",
                "        alpha = torch.softmax(e, dim=-1)\n",
                "        \n",
                "        # Aggregate\n",
                "        h_prime = torch.matmul(alpha, Wh)\n",
                "        return h_prime, alpha  # Return attention weights (edge probabilities!)\n",
                "\n",
                "print(\"GAT Advantages:\")\n",
                "print(\"  1. Attention weights = edge probabilities\")\n",
                "print(\"  2. Explicitly models graph structure\")\n",
                "print(\"  3. Interpretable attention patterns\")\n",
                "print(\"\\n✅ GAT is designed for graph-structured data.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Fix 8: Contrastive Pre-Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pre-train encoder using contrastive learning\n",
                "\n",
                "class ContrastiveLoss(nn.Module):\n",
                "    def __init__(self, temperature=0.5):\n",
                "        super().__init__()\n",
                "        self.temperature = temperature\n",
                "        \n",
                "    def forward(self, z1, z2):\n",
                "        # z1, z2: (batch, embed_dim) - embeddings of positive pairs\n",
                "        \n",
                "        # Normalize\n",
                "        z1 = F.normalize(z1, dim=-1)\n",
                "        z2 = F.normalize(z2, dim=-1)\n",
                "        \n",
                "        # Similarity matrix\n",
                "        sim = torch.matmul(z1, z2.T) / self.temperature\n",
                "        \n",
                "        # Labels: diagonal elements are positive pairs\n",
                "        labels = torch.arange(z1.shape[0]).to(z1.device)\n",
                "        \n",
                "        # Cross-entropy loss\n",
                "        loss = F.cross_entropy(sim, labels)\n",
                "        return loss\n",
                "\n",
                "print(\"Contrastive Pre-Training:\")\n",
                "print(\"  Positive pairs: Same graph, different interventions\")\n",
                "print(\"  Negative pairs: Different graphs\")\n",
                "print(\"\\n✅ Pre-training helps encoder learn better representations.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Fix 9: Benchmark Against Baselines"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare against classical methods\n",
                "\n",
                "from sklearn.metrics import roc_auc_score, f1_score\n",
                "\n",
                "def run_pc_algorithm(X):\n",
                "    \"\"\"Placeholder for PC algorithm\"\"\"\n",
                "    # In practice, use: from causal-learn import pc\n",
                "    # adj_pred = pc(X)\n",
                "    return np.random.randint(0, 2, (X.shape[1], X.shape[1]))\n",
                "\n",
                "def run_notears(X):\n",
                "    \"\"\"Placeholder for NOTEARS\"\"\"\n",
                "    # In practice, use: from notears import notears_linear\n",
                "    # adj_pred = notears_linear(X)\n",
                "    return np.random.rand(X.shape[1], X.shape[1])\n",
                "\n",
                "# Example comparison\n",
                "X_test = np.random.randn(1000, 10)\n",
                "adj_true = np.random.randint(0, 2, (10, 10))\n",
                "\n",
                "adj_pc = run_pc_algorithm(X_test)\n",
                "adj_notears = run_notears(X_test)\n",
                "\n",
                "print(\"Baseline Comparison:\")\n",
                "print(f\"  PC Algorithm: AUROC = {roc_auc_score(adj_true.ravel(), adj_pc.ravel()):.3f}\")\n",
                "print(f\"  NOTEARS: AUROC = {roc_auc_score(adj_true.ravel(), adj_notears.ravel()):.3f}\")\n",
                "print(f\"  Our Model: AUROC = 0.500 (random guessing)\")\n",
                "print(\"\\n⚠️ We need to beat these baselines!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 9. Next Steps\n",
                "\n",
                "### Immediate Actions (This Week)\n",
                "1. ✅ **Implement Focal Loss** → Replace weighted BCE\n",
                "2. ✅ **Increase Learning Rate** → Try 1e-3\n",
                "3. ✅ **Simplify Architecture** → Remove SetEmbedding, use direct mean\n",
                "4. ✅ **Run Short Experiment** → 2000 steps with new config\n",
                "\n",
                "### Short-Term (Next 2 Weeks)\n",
                "5. ✅ **Add Auxiliary Task** → Intervention prediction\n",
                "6. ✅ **Data Augmentation** → Permutation + noise\n",
                "7. ✅ **Progressive Curriculum** → Gradual complexity increase\n",
                "8. ✅ **Hyperparameter Tuning** → Grid search over LR, embed_dim, n_layers\n",
                "\n",
                "### Long-Term (Next Month)\n",
                "9. ✅ **Implement GAT** → Replace Transformer\n",
                "10. ✅ **Contrastive Pre-Training** → Pre-train encoder\n",
                "11. ✅ **Benchmark Baselines** → Compare against PC, GES, NOTEARS\n",
                "12. ✅ **Real-World Evaluation** → Test on Sachs, DREAM datasets\n",
                "\n",
                "### Success Criteria\n",
                "- **AUROC > 0.7** (currently 0.5)\n",
                "- **F1 > 0.3** (currently 0.0)\n",
                "- **SHD < 100** (currently 500+)\n",
                "\n",
                "---\n",
                "\n",
                "## Conclusion\n",
                "\n",
                "We have built a **solid foundation**:\n",
                "- ✅ Clean, modular codebase\n",
                "- ✅ Comprehensive testing and reporting\n",
                "- ✅ Memory-optimized training pipeline\n",
                "\n",
                "But the model **is not learning**:\n",
                "- ❌ AUROC ≈ 0.5 (random guessing)\n",
                "- ❌ F1 = 0.0 (no edges predicted)\n",
                "- ❌ Stuck in local minimum\n",
                "\n",
                "**Root Causes**:\n",
                "1. **Sparsity Trap**: Model exploits class imbalance\n",
                "2. **Architecture Limitations**: SetEmbedding loses information\n",
                "3. **Optimization Issues**: Shallow local minimum\n",
                "\n",
                "**Path Forward**:\n",
                "- **Immediate**: Focal Loss + higher LR + simpler architecture\n",
                "- **Short-term**: Multi-task learning + data augmentation\n",
                "- **Long-term**: GAT + contrastive pre-training\n",
                "\n",
                "**The goal is achievable**, but requires fundamental changes to the approach.\n",
                "\n",
                "---\n",
                "\n",
                "## References\n",
                "1. **NOTEARS**: Zheng et al., \"DAGs with NO TEARS\" (NeurIPS 2018)\n",
                "2. **PC Algorithm**: Spirtes et al., \"Causation, Prediction, and Search\" (2000)\n",
                "3. **Focal Loss**: Lin et al., \"Focal Loss for Dense Object Detection\" (ICCV 2017)\n",
                "4. **GAT**: Veličković et al., \"Graph Attention Networks\" (ICLR 2018)\n",
                "5. **Contrastive Learning**: Chen et al., \"A Simple Framework for Contrastive Learning\" (ICML 2020)\n",
                "\n",
                "---\n",
                "\n",
                "**Project Repository**: [https://github.com/MeiisamMahmoodii/TabSCM](https://github.com/MeiisamMahmoodii/TabSCM)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}